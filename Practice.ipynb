{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d49874-7280-4c06-b98e-3074acbb32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dae4880a-2015-4e05-8d23-24ab54eff197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 1....\n",
      "Downloading page 2....\n"
     ]
    }
   ],
   "source": [
    "class Annapurna_Scraper:\n",
    "    def __init__(self,page_no,search_keyword = 'चुनाव',save_to_file=True):\n",
    "        self.search_keyword = search_keyword\n",
    "        self.page_no = page_no\n",
    "        self.save_to_file = save_to_file\n",
    "        self.data = []\n",
    "        \n",
    "    def fetch_data(self,search_key,page):\n",
    "        url = f\"https://bg.annapurnapost.com/api/search?title={search_key}&page={page}\"\n",
    "        header = {\n",
    "            'authority': 'bg.annapurnapost.com',\n",
    "            'content-type': 'application/json',\n",
    "            'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Google Chrome\";v=\"98\"',\n",
    "            'cache-control': 'no-cache',\n",
    "            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',\n",
    "\n",
    "        }\n",
    "        return requests.get(url,headers=header)\n",
    "    def Scrape(self):\n",
    "        try:\n",
    "            temp = []\n",
    "            s_index = 0\n",
    "            try:\n",
    "                with open('scrape.json','r') as f:\n",
    "                    data_from_file = json.loads(f.read())\n",
    "                file_flag = True\n",
    "            except:\n",
    "                file_flag = False\n",
    "            for page in range(self.page_no):\n",
    "                if file_flag == True:  \n",
    "                    try:  \n",
    "                        for file in data_from_file:\n",
    "                            try:\n",
    "                                if (len(file) == 1 and file['page'] == page+1):\n",
    "                                    f_index = data_from_file.index(file)\n",
    "                                    print(f'page {page+1} already found in the disk.....')\n",
    "                                    temp+=data_from_file[s_index:f_index]\n",
    "                                    s_index = f_index\n",
    "                                    page_flag = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    page_flag = False\n",
    "                            except:\n",
    "                                continue\n",
    "                        if page_flag == True:\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                print(f'Downloading page {page+1}....')\n",
    "                response = self.fetch_data(self.search_keyword,page+1)\n",
    "                if response.status_code != 200 :\n",
    "                    raise Exception('Connection error !!!')\n",
    "                else:\n",
    "                    data = response.json()['data']['items']\n",
    "                    data.append({'page' : page+1}) \n",
    "                    self.data = self.data + data\n",
    "            if self.save_to_file == True:\n",
    "                if len(self.data) == 0:\n",
    "                    return None\n",
    "                self.data+=temp\n",
    "                with open('scrape.json','a',encoding='utf8') as f:\n",
    "                    json.dump(self.data,f)\n",
    "            else:    \n",
    "                return self.data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "scrape = Annapurna_Scraper(search_keyword='कांग्रेस',page_no=2,save_to_file=False)\n",
    "data = scrape.Scrape() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccf33d-425f-458b-b3d6-d2538a732260",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Method 1. List of Dictionaries '''\n",
    "def voting_list(state,ward_no):\n",
    "    payload = {\n",
    "        'state':f'{state}',\n",
    "        'district':'26',\n",
    "        'vdc_mun':'5278',\n",
    "        'ward':f'{ward_no}',\n",
    "        'reg_centre':''\n",
    "            }\n",
    "    url = 'https://voterlist.election.gov.np/bbvrs/view_ward_1.php'\n",
    "    response = requests.post(url,data=payload)\n",
    "    soup = BeautifulSoup(response.text,\"html.parser\") \n",
    "    div = soup.find('div',class_='div_bbvrs_data')\n",
    "    table = div.find(\"table\")\n",
    "    head_table_data = ['voter_id','voter_name','age','gender','wife/husband_name','father/mother_name']\n",
    "    body_table_data = table.tbody.find_all(\"tr\")  \n",
    "    list_data = []\n",
    "    for count in range(len(body_table_data)):\n",
    "        dict_data = {}\n",
    "        # for data in body_table_data[i].find_all(\"td\"):\n",
    "        for i,th in enumerate(head_table_data):\n",
    "            dict_data[th[i]] = body_table_data[count].find_all(\"td\")[i+1].get_text()   \n",
    "        list_data.append(dict_data)\n",
    "    string_list_data = str(list_data)\n",
    "    numbers_map = {'0': '०','1': '१','2': '२','3': '३','4': '४','5': '५','6': '६','7': '७','8': '८','9': '९'}\n",
    "    for eng_num,unicode in numbers_map.items():\n",
    "        string_list_data = string_list_data.replace(eng_num,unicode)\n",
    "    bytestring = string_list_data.encode(\"utf-8\")\n",
    "    json_str = json.dumps(string_list_data,ensure_ascii=False).encode('utf8')\n",
    "    with open(f'voting_list_wards{ward_no}.json','wb',encoding='utf8') as file:\n",
    "        file.write(bytestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef69564e-857a-4e63-816e-50dd26a6786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 1....\n",
      "saving...\n",
      "Downloading page 2....\n",
      "saving...\n",
      "Downloading page 3....\n",
      "saving...\n",
      "Downloading page 4....\n",
      "saving...\n",
      "Downloading page 5....\n",
      "saving...\n",
      "Downloading page 6....\n",
      "saving...\n"
     ]
    }
   ],
   "source": [
    "import json,os,requests\n",
    "\n",
    "'''saves file in a folder on the basis of page.'''\n",
    "class Annapurna_Scraper:\n",
    "    '''\n",
    "        Annapurna posts scraper: \n",
    "            It scrapes the posts on the given search_keyword parameter page by page\n",
    "            Scrape up to arround 20 pages at a time; \n",
    "        \n",
    "        How to use:\n",
    "        -- Create an instance of Annapurna_Scraper class\n",
    "        -- While creating instance provide three parameters shown below\n",
    "        -- Call Scrape() method with the instance\n",
    "        -- This method returns data but if save_to_file=True then it also saves to 'scrape.json' \n",
    "            file on the current directory \n",
    "        \n",
    "        >>>> s = Annapurna_Scraper(,page_no=5,search_keyword = 'चुनाव',save_to_file=True)\n",
    "        >>>> data = s.Scrape()\n",
    "    '''\n",
    "    def __init__(self,page_no,search_keyword = 'चुनाव',save_to_file=True):\n",
    "        self.search_keyword = search_keyword\n",
    "        self.page_no = page_no\n",
    "        self.save_to_file = save_to_file\n",
    "        self.response_data = []\n",
    "        \n",
    "    def request_data(self,page):\n",
    "        url = f\"https://bg.annapurnapost.com/api/search?title={self.search_keyword}&page={page}\"\n",
    "        header = {\n",
    "            'authority': 'bg.annapurnapost.com',\n",
    "            'content-type': 'application/json',\n",
    "            'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Google Chrome\";v=\"98\"',\n",
    "            'cache-control': 'no-cache',\n",
    "            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',\n",
    "\n",
    "        }\n",
    "        return requests.get(url,headers=header)\n",
    "    \n",
    "    def fetch_data(self,page):\n",
    "        print(f'Downloading page {page}....')\n",
    "        response = self.request_data(page)\n",
    "        if response.status_code != 200 :\n",
    "            raise Exception('Connection error !!!')\n",
    "        else:\n",
    "            return response.json()['data']['items']\n",
    "            \n",
    "    def save_data(self,data,page):\n",
    "        print('saving...')\n",
    "        if not os.path.exists(f'annapurna-scrape/{self.search_keyword}'):\n",
    "            os.mkdir(f\"annapurna-scrape/{self.search_keyword}\")\n",
    "        with open(f'annapurna-scrape/{self.search_keyword}/{page}.json','w',encoding='utf8') as f:\n",
    "            json.dump(data,f,ensure_ascii=False)\n",
    "        return data\n",
    "\n",
    "    def Scrape(self): \n",
    "        for page in range(1,self.page_no+1):\n",
    "            try:\n",
    "                with open(f'annapurna-scrape/{self.search_keyword}/{page}.json','r') as f:\n",
    "                    data_from_file = json.loads(f.read())\n",
    "                file_flag = True\n",
    "            except:\n",
    "                file_flag = False\n",
    "            \n",
    "            if file_flag == True:\n",
    "                if len(data_from_file) != 0 :\n",
    "                    print(f'page {page} already found in the disk.....')\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                data = self.fetch_data(page)\n",
    "                self.response_data.append(self.save_data(data,page))\n",
    "                    \n",
    "        if self.save_to_file == True:\n",
    "            return None\n",
    "        else:\n",
    "            return self.response_data\n",
    "\n",
    "scrape = Annapurna_Scraper(search_keyword='कांग्रेस',page_no=6,save_to_file=True)\n",
    "data = scrape.Scrape() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6aec21-3c4e-4264-a12e-2a4df2a701d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f1cf8-1fcc-43fa-8129-8182d28353e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
